{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "v4Vi0xfFBBkM",
        "outputId": "6c3051aa-f3eb-4831-a711-1a19900444a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 112375898.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20341846.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27703458.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3361067.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor epoch in range(num_epochs):\\n    for i, (images, _) in enumerate(loaders['train']):\\n        images = images.to(device)  # Move data to the GPU if available\\n        optimizer.zero_grad()\\n        outputs = model(images)\\n        loss = loss_func(outputs, images)\\n        loss.backward()\\n        optimizer.step()\\n        if (i+1) % 100 == 0:\\n            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\\n                  % (epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\\n\\n# Testing\\nmodel.eval()\\nfor images, _ in loaders['test']:\\n    images = images.to(device)  # Move data to the GPU if available\\n    outputs = model(images)\\n    break\\n\\n# Move outputs and images back to CPU for visualization (if necessary)\\noutputs = outputs.cpu().data.numpy()\\nimages = images.cpu().data.numpy()\\n\\nprint (outputs.shape)\\nprint (images.shape)\\nn = 10\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    ax = plt.subplot(2, n, i+1)\\n    plt.imshow(images[i][0], cmap='gray')\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    ax = plt.subplot(2, n, i+1+n)\\n    plt.imshow(outputs[i][0], cmap='gray')\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n### Save the model checkpoint\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "print (train_data)\n",
        "print (test_data)\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "learning_rate = 0.001\n",
        "from torch.utils.data import DataLoader\n",
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(train_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1),\n",
        "\n",
        "    'test'  : torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=1),\n",
        "}\n",
        "loaders\n",
        "### Encoder\n",
        "#creation using\n",
        "\n",
        "#Conv2D, MaxPooling2D,Conv2D, MaxPooling2D,Conv2D, MaxPooling2D\n",
        "###\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "###Decoder creation using:\n",
        "\n",
        "#Conv2D, UpSampling2D,Conv2D, UpSampling2D,Conv2D, UpSampling2D,Final Conv2D to get image\n",
        "\n",
        "class Decoder (nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3,stride=2, padding=1, output_padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3,stride=2, padding=1, output_padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=3,stride=2, padding=0, output_padding=1)\n",
        "        self.conv4 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3,padding=0)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.batch_norm = nn.BatchNorm2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "       # x = self.upsample(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "       # x = self.upsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "      #  x = self.upsample(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = nn.Sigmoid()(x)\n",
        "        #x = self.batch_norm(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "### Autoencoder creation using:\n",
        "# Encoder, Decoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Move your model to the device (GPU if available, else CPU)\n",
        "model = Autoencoder().to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        " #loss_func = nn.CrossEntropyLoss()\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch import optim\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        " ### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_loss_train = []\n",
        "list_accuracy_train = []\n",
        "list_loss_test = []\n",
        "list_accuracy_test = []\n",
        "\n",
        "y_pred =[]\n",
        "y_true =[]\n",
        "from torch.autograd import Variable\n",
        "seuil=0.5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    sous_list_loss_train = []\n",
        "    sous_valeur_accuracy_train = 0\n",
        "    #for i, (inputs_train, labels_train ) in enumerate(loaders['train']):\n",
        "    for i, (images, labels_train) in enumerate(loaders['train']):\n",
        "        images = images.to(device)  # Move data to the GPU if available\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
        "                  % (epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "\n",
        "\n",
        "        sous_list_loss_train.append(loss.item())\n",
        "        sous_valeur_accuracy_train =(outputs > 0.5).float().eq(images > 0.5).sum().item()\n",
        "    accuracy = (sous_valeur_accuracy_train / images.numel()) * 100\n",
        "    print(accuracy )\n",
        "    #total_pixels = images.view(images.size(0), -1).size(1)\n",
        "    #list_accuracy_train.append(sous_valeur_accuracy_train /( len(loaders['train']) * total_pixels))\n",
        "    list_accuracy_train.append(accuracy)\n",
        "    list_loss_train.append(np.mean(sous_list_loss_train))\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sous_list_loss_test = []\n",
        "        sous_valeur_accuracy_test = 0\n",
        "        for images, labels_test in loaders['test']:\n",
        "            images = images.to(device)  # Move data to the GPU if available\n",
        "            outputs = model(images)\n",
        "\n",
        "\n",
        "            loss = loss_func(outputs,  images)\n",
        "            sous_list_loss_test.append(loss.item())\n",
        "            sous_valeur_accuracy_test =  (outputs > 0.5).float().eq(images > 0.5).sum().item()\n",
        "        accuracy_test = (sous_valeur_accuracy_test / images.numel()) * 100\n",
        "        print(accuracy_test)\n",
        "        list_accuracy_test.append(accuracy_test)\n",
        "\n",
        "        list_loss_test.append(np.mean(sous_list_loss_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okKIIcP9P6fp",
        "outputId": "d06044a3-f8c9-4b66-ebf6-f8026c1db6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Iter [100/468] Loss: 0.1237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "outputs = outputs.cpu().data.numpy()\n",
        "images = images.cpu().data.numpy()\n",
        "\n",
        "print (outputs.shape)\n",
        "print (images.shape)\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(2, n, i+1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    ax = plt.subplot(2, n, i+1+n)\n",
        "    plt.imshow(outputs[i][0], cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V566n41EU7J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(list_accuracy_train, label='Training accuracy')\n",
        "#plt.show()\n",
        "plt.plot(list_accuracy_test, label='Test accuracy')\n",
        "plt.legend()\n",
        "\n",
        "    # Show the figure\n",
        "plt.show()\n",
        "plt.plot(list_loss_train, label='Training loss')\n",
        "plt.plot(list_loss_test , label='Test loss')\n",
        "plt.legend()\n",
        "    # Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qJfhDkj1UpPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}